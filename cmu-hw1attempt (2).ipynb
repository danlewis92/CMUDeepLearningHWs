{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":80670,"databundleVersionId":8695879,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW1: Frame-Level Speech Recognition","metadata":{"id":"F9ERgBpbcMmB"}},{"cell_type":"markdown","source":"In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame.","metadata":{"id":"CLkH6GMGcWcE"}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"z4vZbDmJvMp1"}},{"cell_type":"code","source":"!pip install torchsummaryX==1.1.0 wandb --quiet","metadata":{"id":"rwYu9sSUnSho","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:14.592503Z","iopub.execute_input":"2025-01-20T09:11:14.592950Z","iopub.status.idle":"2025-01-20T09:11:22.927735Z","shell.execute_reply.started":"2025-01-20T09:11:14.592893Z","shell.execute_reply":"2025-01-20T09:11:22.926778Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# To ensure package compatibility\n!pip install numpy==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:22.929602Z","iopub.execute_input":"2025-01-20T09:11:22.929891Z","iopub.status.idle":"2025-01-20T09:11:30.973304Z","shell.execute_reply.started":"2025-01-20T09:11:22.929862Z","shell.execute_reply":"2025-01-20T09:11:30.972247Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchsummaryX import summary\nimport sklearn\nimport gc\nimport zipfile\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport os\nimport datetime\nimport wandb\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)","metadata":{"id":"qI4qfx7tiBZt","outputId":"ef79a3fc-5689-4e5a-d896-329b8a9d6a5c","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:30.974801Z","iopub.execute_input":"2025-01-20T09:11:30.975099Z","iopub.status.idle":"2025-01-20T09:11:35.721058Z","shell.execute_reply.started":"2025-01-20T09:11:30.975070Z","shell.execute_reply":"2025-01-20T09:11:35.720146Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"### PHONEME LIST\nPHONEMES = [\n            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']","metadata":{"id":"N-9qE20hmCgQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.722138Z","iopub.execute_input":"2025-01-20T09:11:35.722443Z","iopub.status.idle":"2025-01-20T09:11:35.727173Z","shell.execute_reply.started":"2025-01-20T09:11:35.722415Z","shell.execute_reply":"2025-01-20T09:11:35.726288Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Kaggle","metadata":{"id":"ZIi0Big7vPa9"}},{"cell_type":"markdown","source":"This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully.","metadata":{"id":"BBCbeRhixGM7"}},{"cell_type":"code","source":"#!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n#!mkdir /root/.kaggle\n\n#with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n#    f.write('{\"username\":\"USERNAME\",\"key\":\"KEY\"}')\n    # Put your kaggle username & key here\n\n#!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"TPBUd7Cnl-Rx","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.730136Z","iopub.execute_input":"2025-01-20T09:11:35.730489Z","iopub.status.idle":"2025-01-20T09:11:35.746826Z","shell.execute_reply.started":"2025-01-20T09:11:35.730451Z","shell.execute_reply":"2025-01-20T09:11:35.746028Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# commands to download data from kaggle\n#!kaggle competitions download -c 11785-hw1p2-f24\n\n#!unzip -qo /content/11785-hw1p2-f24.zip -d '/content'","metadata":{"id":"if2Somqfbje1","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.747809Z","iopub.execute_input":"2025-01-20T09:11:35.748125Z","iopub.status.idle":"2025-01-20T09:11:35.758134Z","shell.execute_reply.started":"2025-01-20T09:11:35.748088Z","shell.execute_reply":"2025-01-20T09:11:35.757413Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#os.listdir()\n#!unzip 11785-hw1p2-f24.zip -d /dev/shm","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.759300Z","iopub.execute_input":"2025-01-20T09:11:35.759637Z","iopub.status.idle":"2025-01-20T09:11:35.769300Z","shell.execute_reply.started":"2025-01-20T09:11:35.759602Z","shell.execute_reply":"2025-01-20T09:11:35.768487Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"Vuzce0_TdcaR"}},{"cell_type":"code","source":"os.getcwd()\nos.chdir(\"/kaggle/input/11785-hw1p2-f24\")\nos.listdir()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.770332Z","iopub.execute_input":"2025-01-20T09:11:35.770624Z","iopub.status.idle":"2025-01-20T09:11:35.784743Z","shell.execute_reply.started":"2025-01-20T09:11:35.770599Z","shell.execute_reply":"2025-01-20T09:11:35.783997Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['11785-f24-hw1p2']"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Dataset class to load train and validation data\nclass AudioDataset(torch.utils.data.Dataset):\n    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n        self.context    = context\n        self.phonemes   = phonemes\n        # MFCC directory - use partition to access train/dev directories from kaggle data using root\n        self.mfcc_dir       = os.path.join(root,partition,\"mfcc/\")\n        # Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n        self.transcript_dir = os.path.join(root,partition,\"transcript/\")\n        # List files in sefl.mfcc_dir using os.listdir in sorted order\n        mfcc_names          = os.listdir(self.mfcc_dir)\n        mfcc_names.sort()\n        # List files in self.transcript_dir using os.listdir in sorted order\n        transcript_names    = os.listdir(self.transcript_dir)\n        transcript_names.sort()\n        # Making sure that we have the same no. of mfcc and transcripts\n        assert len(mfcc_names) == len(transcript_names)\n        self.mfccs, self.transcripts = [], []\n        # Iterate through mfccs and transcripts\n        for i in range(len(mfcc_names)):\n        #   Load a single mfcc\n            mfcc        = np.load(os.path.join(self.mfcc_dir,mfcc_names[i]))\n        #   Do Cepstral Normalization of mfcc (explained in writeup)\n            mfcc -= (np.mean(mfcc, axis=0))\n            mfcc /= (np.std(mfcc, axis=0))       \n        #   Load the corresponding transcript\n            transcript  = np.load(os.path.join(self.transcript_dir,transcript_names[i]))\n        # Remove [SOS] and [EOS] from the transcript\n            transcript = transcript[1:len(transcript)-1]\n        #   Append each mfcc to self.mfcc, transcript to self.transcript\n            self.mfccs.append(mfcc)\n            self.transcripts.append(transcript)\n        # NOTE:\n        # Each mfcc is of shape T1 x 28, T2 x 28, ...\n        # Each transcript is of shape (T1+2), (T2+2) before removing [SOS] and [EOS]\n        # Concatenate all mfccs in self.mfccs such that\n        # the final shape is T x 28 (Where T = T1 + T2 + ...)\n        self.mfccs          = np.concatenate(self.mfccs)\n        # Concatenate all transcripts in self.transcripts such that\n        # the final shape is (T,) meaning, each time step has one phoneme output\n        self.transcripts    = np.concatenate(self.transcripts)\n        # Length of the dataset is now the length of concatenated mfccs/transcripts\n        self.length = len(self.mfccs)\n        # Take some time to think about what we have done.\n        # self.mfcc is an array of the format (Frames x Features).\n        # Our goal is to recognize phonemes of each frame\n        # We can introduce context by padding zeros on top and bottom of self.mfcc\n        Z = np.zeros((context,28),dtype='float32')\n        self.mfccs = np.concatenate([Z,self.mfccs,Z])\n        # The available phonemes in the transcript are of string data type\n        # But the neural network cannot predict strings as such.\n        # Hence, we map these phonemes to integers\n        # Map the phonemes to their corresponding list indexes in self.phonemes\n        self.transcripts = np.array([PHONEMES.index(self.transcripts[i]) for i in range(len(self.transcripts))])\n        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        # Based on context and offset, return a frame at given index with context frames to the left, and right.\n        frames      = self.mfccs[ind:ind+2*self.context+1]\n        # After slicing, you get an array of shape 2*context+1 x 28. But our MLP needs 1d data and not 2d.\n        frames      = frames.flatten() # Flatten to get 1d data\n        frames      = torch.FloatTensor(frames) # Convert to tensors\n        phonemes    = torch.tensor(self.transcripts[ind])\n        return frames, phonemes","metadata":{"id":"YpLCvi3AJC5z","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.785927Z","iopub.execute_input":"2025-01-20T09:11:35.786244Z","iopub.status.idle":"2025-01-20T09:11:35.799954Z","shell.execute_reply.started":"2025-01-20T09:11:35.786212Z","shell.execute_reply":"2025-01-20T09:11:35.799225Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class AudioTestDataset(torch.utils.data.Dataset):\n    #pass\n    # Create a test dataset class similar to the previous class but you dont have transcripts for this\n    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n\n        self.context = context\n        self.phonemes = phonemes\n        self.mfcc_dir = os.path.join(root,partition,\"mfcc/\")\n\n        mfcc_names = os.listdir(self.mfcc_dir)\n        mfcc_names.sort()\n        self.mfccs = []\n\n        for i in range(len(mfcc_names)):\n            mfcc = np.load(os.path.join(self.mfcc_dir,mfcc_names[i]))\n            mfcc -= (np.mean(mfcc, axis=0))\n            mfcc /= (np.std(mfcc, axis=0))     \n            self.mfccs.append(mfcc)\n \n        self.mfccs = np.concatenate(self.mfccs)\n        self.length = len(self.mfccs)\n        Z = np.zeros((context,28),dtype='float32')\n        self.mfccs = np.concatenate([Z,self.mfccs,Z])\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        frames = self.mfccs[ind:ind+2*self.context+1]\n        frames = frames.flatten()\n        frames = torch.FloatTensor(frames)\n        return frames","metadata":{"id":"e8KfVP39S6o7","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.800997Z","iopub.execute_input":"2025-01-20T09:11:35.801292Z","iopub.status.idle":"2025-01-20T09:11:35.815905Z","shell.execute_reply.started":"2025-01-20T09:11:35.801258Z","shell.execute_reply":"2025-01-20T09:11:35.815076Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Parameters Configuration","metadata":{"id":"qNacQ8bpt9nw"}},{"cell_type":"markdown","source":"Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments.","metadata":{"id":"WE7tsinAuLNy"}},{"cell_type":"code","source":"# CONTEXT default: 20\n# batch_size default: 1024\n# init_lr default: 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.816916Z","iopub.execute_input":"2025-01-20T09:11:35.817215Z","iopub.status.idle":"2025-01-20T09:11:35.831706Z","shell.execute_reply.started":"2025-01-20T09:11:35.817182Z","shell.execute_reply":"2025-01-20T09:11:35.830962Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"config = {\n    'epochs'        : 5,\n    'batch_size'    : 512,\n    'context'       : 20,\n    'init_lr'       : 1e-3,\n    'architecture'  : 'aiming-high-cutoff',\n    'drop'          : 0,\n    'checkpoint_dir': \"/kaggle/working\"\n    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n}","metadata":{"id":"PmKwlFqgt_Zq","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.832786Z","iopub.execute_input":"2025-01-20T09:11:35.833097Z","iopub.status.idle":"2025-01-20T09:11:35.843216Z","shell.execute_reply.started":"2025-01-20T09:11:35.833062Z","shell.execute_reply":"2025-01-20T09:11:35.842398Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Create Datasets","metadata":{"id":"2mlwaKlDt_2c"}},{"cell_type":"code","source":"# Create a dataset object using the AudioDataset class for the training data\ntrain_data = AudioDataset(\"11785-f24-hw1p2/\", context=config['context'])\n# Create a dataset object using the AudioDataset class for the validation data\nval_data = AudioDataset(\"11785-f24-hw1p2/\", context=config['context'], partition=\"dev-clean\")\n# Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(\"11785-f24-hw1p2/\", context=config['context'], partition=\"test-clean\")","metadata":{"id":"7xi7V8x8W9z4","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:11:35.844134Z","iopub.execute_input":"2025-01-20T09:11:35.844416Z","iopub.status.idle":"2025-01-20T09:22:44.824425Z","shell.execute_reply.started":"2025-01-20T09:11:35.844381Z","shell.execute_reply":"2025-01-20T09:22:44.823617Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader.\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 4,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = True\n)\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\nprint(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\nprint(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))","metadata":{"id":"4mzoYfTKu14s","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:44.827547Z","iopub.execute_input":"2025-01-20T09:22:44.827810Z","iopub.status.idle":"2025-01-20T09:22:44.835607Z","shell.execute_reply.started":"2025-01-20T09:22:44.827785Z","shell.execute_reply":"2025-01-20T09:22:44.834702Z"}},"outputs":[{"name":"stdout","text":"Batch size     :  512\nContext        :  20\nInput size     :  1148\nOutput symbols :  42\nTrain dataset samples = 36091157, batches = 70491\nValidation dataset samples = 1928204, batches = 3767\nTest dataset samples = 1934138, batches = 3778\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n    break","metadata":{"id":"n-GV3UvgLSoF","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:44.836654Z","iopub.execute_input":"2025-01-20T09:22:44.836941Z","iopub.status.idle":"2025-01-20T09:22:50.423079Z","shell.execute_reply.started":"2025-01-20T09:22:44.836903Z","shell.execute_reply":"2025-01-20T09:22:50.422265Z"}},"outputs":[{"name":"stdout","text":"torch.Size([512, 1148]) torch.Size([512])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Network Architecture\n","metadata":{"id":"Nxjwve20JRJ2"}},{"cell_type":"markdown","source":"This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline.","metadata":{"id":"3NJzT-mRw6iy"}},{"cell_type":"code","source":"# This architecture will make you cross the very low cutoff\n# However, you need to run a lot of experiments to cross the medium or high cutoff\nclass Network(torch.nn.Module):\n\n    def __init__(self, input_size, output_size):\n\n        super(Network, self).__init__()\n\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(input_size, 360),\n            torch.nn.ReLU(),\n            torch.nn.Linear(360, 360),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(config['drop']),\n            torch.nn.Linear(360, output_size)\n        )\n\n    def forward(self, x):\n        out = self.model(x)\n\n        return out","metadata":{"id":"OvcpontXQq9j","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.424455Z","iopub.execute_input":"2025-01-20T09:22:50.424738Z","iopub.status.idle":"2025-01-20T09:22:50.430770Z","shell.execute_reply.started":"2025-01-20T09:22:50.424709Z","shell.execute_reply":"2025-01-20T09:22:50.429931Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Define Model, Loss Function and Optimizer","metadata":{"id":"HejoSXe3vMVU"}},{"cell_type":"markdown","source":"Here we define the model, loss function, optimizer and optionally a learning rate scheduler.","metadata":{"id":"xAhGBH7-xxth"}},{"cell_type":"code","source":"INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\nmodel       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\nsummary(model, frames.to(device))\n# Check number of parameters of your network\n# Remember, you are limited to 20 million parameters for HW1 (including ensembles)","metadata":{"id":"_qtrEM1ZvLje","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.431688Z","iopub.execute_input":"2025-01-20T09:22:50.431900Z","iopub.status.idle":"2025-01-20T09:22:50.598895Z","shell.execute_reply.started":"2025-01-20T09:22:50.431877Z","shell.execute_reply":"2025-01-20T09:22:50.598048Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nLayer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n====================================================================================================\n0_Linear                 [1148, 360]           [512, 360]               413.64                 0.41\n1_ReLU                             -           [512, 360]                    -                    -\n2_Linear                  [360, 360]           [512, 360]               129.96                 0.13\n3_ReLU                             -           [512, 360]                    -                    -\n4_Dropout                          -           [512, 360]                    -                    -\n5_Linear                   [360, 42]            [512, 42]                15.16                 0.02\n====================================================================================================\n# Params:    558.76K\n# Mult-Adds: 0.56M\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"INPUT_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.600121Z","iopub.execute_input":"2025-01-20T09:22:50.600564Z","iopub.status.idle":"2025-01-20T09:22:50.606386Z","shell.execute_reply.started":"2025-01-20T09:22:50.600529Z","shell.execute_reply":"2025-01-20T09:22:50.605615Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1148"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"len(train_data.phonemes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.607858Z","iopub.execute_input":"2025-01-20T09:22:50.608194Z","iopub.status.idle":"2025-01-20T09:22:50.619536Z","shell.execute_reply.started":"2025-01-20T09:22:50.608158Z","shell.execute_reply":"2025-01-20T09:22:50.618758Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.620629Z","iopub.execute_input":"2025-01-20T09:22:50.620896Z","iopub.status.idle":"2025-01-20T09:22:50.629672Z","shell.execute_reply.started":"2025-01-20T09:22:50.620854Z","shell.execute_reply":"2025-01-20T09:22:50.628919Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Network(\n  (model): Sequential(\n    (0): Linear(in_features=1148, out_features=360, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=360, out_features=360, bias=True)\n    (3): ReLU()\n    (4): Dropout(p=0, inplace=False)\n    (5): Linear(in_features=360, out_features=42, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n# We use CE because the task is multi-class classification\n\noptimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n# Recommended : Define Scheduler for Learning Rate,\n# including but not limited to StepLR, MultiStep, CosineAnnealing, CosineAnnealingWithWarmRestarts, ReduceLROnPlateau, etc.\n# You can refer to Pytorch documentation for more information on how to use them.\n\n# Defining Scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n\n# Is your training time very high?\n# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\nscaler = torch.amp.GradScaler('cuda')","metadata":{"id":"UROGEVJevKD-","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:50.630623Z","iopub.execute_input":"2025-01-20T09:22:50.630866Z","iopub.status.idle":"2025-01-20T09:22:51.472231Z","shell.execute_reply.started":"2025-01-20T09:22:50.630841Z","shell.execute_reply":"2025-01-20T09:22:51.471574Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Training and Validation Functions","metadata":{"id":"IBwunYpyugFg"}},{"cell_type":"markdown","source":"This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. This code was provided.","metadata":{"id":"1JgeNhx4x2-P"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"XblOHEVtKab2","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:51.473140Z","iopub.execute_input":"2025-01-20T09:22:51.473569Z","iopub.status.idle":"2025-01-20T09:22:51.622108Z","shell.execute_reply.started":"2025-01-20T09:22:51.473539Z","shell.execute_reply":"2025-01-20T09:22:51.621189Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def train(model, dataloader, optimizer, lr_scheduler, criterion, scaler):\n    model.train()\n    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n    for i, (frames, phonemes) in enumerate(dataloader):\n        ### Initialize Gradients\n        optimizer.zero_grad()\n        ### Move Data to Device (Ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n         # forward\n        with torch.amp.autocast('cuda'):  # This implements mixed precision. Thats it!\n        ### Forward Propagation\n            logits  = model(frames)\n        ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n        ### Backward Propagation\n        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n        ### Gradient Descent\n        scaler.step(optimizer) # This is a replacement for optimizer.step()\n        scaler.update()\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n        tloss   += loss.item()\n        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n                             acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n        batch_bar.update()\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n    batch_bar.close()\n    tloss   /= len(train_loader)\n    tacc    /= len(train_loader)\n    return tloss, tacc","metadata":{"id":"8wjPz7DHqKcL","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:51.623222Z","iopub.execute_input":"2025-01-20T09:22:51.623567Z","iopub.status.idle":"2025-01-20T09:22:51.633628Z","shell.execute_reply.started":"2025-01-20T09:22:51.623540Z","shell.execute_reply":"2025-01-20T09:22:51.632898Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def eval(model, dataloader):\n    model.eval() # set model in evaluation mode\n    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n    for i, (frames, phonemes) in enumerate(dataloader):\n        ### Move data to device (ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n        # makes sure that there are no gradients computed as we are not training the model now\n        with torch.inference_mode():\n            ### Forward Propagation\n            logits  = model(frames)\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n        vloss   += loss.item()\n        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n        batch_bar.update()\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n    batch_bar.close()\n    vloss   /= len(val_loader)\n    vacc    /= len(val_loader)\n    return vloss, vacc","metadata":{"id":"Q5npQNFH315V","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:51.634483Z","iopub.execute_input":"2025-01-20T09:22:51.634712Z","iopub.status.idle":"2025-01-20T09:22:51.648663Z","shell.execute_reply.started":"2025-01-20T09:22:51.634689Z","shell.execute_reply":"2025-01-20T09:22:51.647994Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Weights and Biases Setup","metadata":{"id":"yMd_XxPku5qp"}},{"cell_type":"code","source":"wandb.login(key=\"KEY\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"id":"SCDYx5VEu6qI","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:51.649689Z","iopub.execute_input":"2025-01-20T09:22:51.650049Z","iopub.status.idle":"2025-01-20T09:22:52.814115Z","shell.execute_reply.started":"2025-01-20T09:22:51.650012Z","shell.execute_reply":"2025-01-20T09:22:52.813227Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/input/11785-hw1p2-f24/wandb/ wasn't writable, using system temp directory.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/input/11785-hw1p2-f24/wandb/ wasn't writable, using system temp directory\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name    = \"1hiddenlayer-lr1e-3-nodropout-context20FIXED-scaler\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw1p2\", ### Project should be created in your wandb account\n    config  = config ### Wandb Config for your run\n)","metadata":{"id":"xvUnYd3Bw2up","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:52.815116Z","iopub.execute_input":"2025-01-20T09:22:52.815582Z","iopub.status.idle":"2025-01-20T09:22:55.710675Z","shell.execute_reply.started":"2025-01-20T09:22:52.815550Z","shell.execute_reply":"2025-01-20T09:22:55.709999Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanlewis92\u001b[0m (\u001b[33mdanlewis92-university-of-arizona\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113702977777647, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6441f75a6f41adb5bd0e44c3d9f1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20250120_092252-9eva7u3h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2/runs/9eva7u3h' target=\"_blank\">1hiddenlayer-lr1e-3-nodropout-context20FIXED-scaler</a></strong> to <a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2' target=\"_blank\">https://wandb.ai/danlewis92-university-of-arizona/hw1p2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2/runs/9eva7u3h' target=\"_blank\">https://wandb.ai/danlewis92-university-of-arizona/hw1p2/runs/9eva7u3h</a>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"cd \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:55.711709Z","iopub.execute_input":"2025-01-20T09:22:55.711974Z","iopub.status.idle":"2025-01-20T09:22:55.718853Z","shell.execute_reply.started":"2025-01-20T09:22:55.711945Z","shell.execute_reply":"2025-01-20T09:22:55.718029Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model)\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"id":"wft15E_IxYFi","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:55.719773Z","iopub.execute_input":"2025-01-20T09:22:55.720042Z","iopub.status.idle":"2025-01-20T09:22:55.737642Z","shell.execute_reply.started":"2025-01-20T09:22:55.720016Z","shell.execute_reply":"2025-01-20T09:22:55.736867Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['/tmp/wandb/run-20250120_092252-9eva7u3h/files/model_arch.txt']"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"def save_model(model, optimizer, scheduler, train_acc, train_loss, val_acc, val_loss, curr_lr, epoch, path):\n    torch.save(\n        {'model_state_dict'         : model.state_dict(),\n         'optimizer_state_dict'     : optimizer.state_dict(),\n         'scheduler_state_dict'     : scheduler.state_dict(),\n         'train_acc'                : train_acc,\n         'train_loss'               : train_loss,\n         'val_acc'                  : val_acc,\n         'val_loss'                 : val_loss,\n         'lr'                       : curr_lr,\n         'epoch'                    : epoch},\n         path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:55.738742Z","iopub.execute_input":"2025-01-20T09:22:55.739082Z","iopub.status.idle":"2025-01-20T09:22:55.754756Z","shell.execute_reply.started":"2025-01-20T09:22:55.739043Z","shell.execute_reply":"2025-01-20T09:22:55.753935Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"nclx_04fu7Dd"}},{"cell_type":"markdown","source":"Now, it is time to finally run your ablations! Have fun!","metadata":{"id":"MdLMWfEpyGOB"}},{"cell_type":"code","source":"# Iterate over number of epochs to train and evaluate your model\ntorch.cuda.empty_cache()\ngc.collect()\nwandb.watch(model, log=\"all\")\nbest_val_acc = 0.0\neval_cls = True\nfor epoch in range(config['epochs']):\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc   = train(model, train_loader, optimizer, scheduler, criterion, scaler)\n    val_loss, val_acc       = eval(model, val_loader)\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n    ### Log metrics at each epoch in your run\n    # Optionally, you can log at each batch inside train/eval functions\n    # (explore wandb documentation/wandb recitation)\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n    if eval_cls:\n            if val_acc >= best_val_acc:\n                best_val_acc = val_acc\n                save_model(model, optimizer, scheduler, train_acc*100, train_loss, val_acc*100, val_loss, curr_lr, epoch, os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n                wandb.save(os.path.join(config['checkpoint_dir'], 'best_cls.pth'), '/kaggle/')\n                print(\"Saved best classification model\")","metadata":{"id":"MG4F77Nm0Am9","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:22:55.755825Z","iopub.execute_input":"2025-01-20T09:22:55.756292Z","iopub.status.idle":"2025-01-20T10:19:07.332303Z","shell.execute_reply.started":"2025-01-20T09:22:55.756251Z","shell.execute_reply":"2025-01-20T10:19:07.331379Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/70491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/3767 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Acc 70.7099%\tTrain Loss 0.9309\t Learning Rate 0.0010000\n\tVal Acc 71.9959%\tVal Loss 0.8844\nSaved best classification model\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/70491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/3767 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"\tTrain Acc 73.8165%\tTrain Loss 0.8262\t Learning Rate 0.0000199\n\tVal Acc 72.7961%\tVal Loss 0.8592\nSaved best classification model\n\nEpoch 3/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/70491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/3767 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Acc 74.5693%\tTrain Loss 0.8017\t Learning Rate 0.0009222\n\tVal Acc 73.4067%\tVal Loss 0.8406\nSaved best classification model\n\nEpoch 4/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/70491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/3767 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Acc 74.9778%\tTrain Loss 0.7886\t Learning Rate 0.0001693\n\tVal Acc 73.8547%\tVal Loss 0.8255\nSaved best classification model\n\nEpoch 5/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/70491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/3767 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Acc 75.2471%\tTrain Loss 0.7802\t Learning Rate 0.0007129\n\tVal Acc 73.8122%\tVal Loss 0.8271\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Testing and submission to Kaggle","metadata":{"id":"_kXwf5YUo_4A"}},{"cell_type":"markdown","source":"Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function.","metadata":{"id":"WI1hSFYLpJvH"}},{"cell_type":"code","source":"def test(model, test_loader):\n    model.eval() \n    ### List to store predicted phonemes of test data\n    test_predictions = []\n    ### Using inference mode to avoid gradients\n    with torch.inference_mode(): \n        for i, mfccs in enumerate(tqdm(test_loader)):\n            mfccs   = mfccs.to(device)\n            logits  = model(mfccs)\n            ### Get most likely predicted phoneme with argmax\n            predicted_phonemes = torch.argmax(logits, dim= 1) \n            test_predictions.append(predicted_phonemes)\n    return test_predictions","metadata":{"id":"R-SU9fZ3xHtk","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:07.333955Z","iopub.execute_input":"2025-01-20T10:19:07.334436Z","iopub.status.idle":"2025-01-20T10:19:07.341212Z","shell.execute_reply.started":"2025-01-20T10:19:07.334301Z","shell.execute_reply":"2025-01-20T10:19:07.340338Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"predictions = test(model, test_loader)","metadata":{"id":"wG9v6Xmxu7wp","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:07.342439Z","iopub.execute_input":"2025-01-20T10:19:07.343115Z","iopub.status.idle":"2025-01-20T10:19:27.237037Z","shell.execute_reply.started":"2025-01-20T10:19:07.343073Z","shell.execute_reply":"2025-01-20T10:19:27.236020Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3778 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a10b5c7b49d4ff8834b752bdf83523b"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"### Create CSV file with predictions\nwith open(\"./submission.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(predictions)):\n        f.write(\"{},{}\\n\".format(i, predictions[i]))","metadata":{"id":"ZE1hRnvf0bFz","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:27.239292Z","iopub.execute_input":"2025-01-20T10:19:27.240380Z","iopub.status.idle":"2025-01-20T10:19:51.383839Z","shell.execute_reply.started":"2025-01-20T10:19:27.240312Z","shell.execute_reply":"2025-01-20T10:19:51.382810Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"### Finish your wandb run\nrun.finish()","metadata":{"id":"6Wf-P25TXU0N","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:51.384855Z","iopub.execute_input":"2025-01-20T10:19:51.385116Z","iopub.status.idle":"2025-01-20T10:19:53.535518Z","shell.execute_reply.started":"2025-01-20T10:19:51.385089Z","shell.execute_reply":"2025-01-20T10:19:53.534835Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='6.433 MB of 6.433 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█▁▇▂▆</td></tr><tr><td>train_acc</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆██</td></tr><tr><td>valid_loss</td><td>█▅▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.00071</td></tr><tr><td>train_acc</td><td>75.24713</td></tr><tr><td>train_loss</td><td>0.78022</td></tr><tr><td>val_acc</td><td>73.81217</td></tr><tr><td>valid_loss</td><td>0.8271</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">1hiddenlayer-lr1e-3-nodropout-context20FIXED-scaler</strong> at: <a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2/runs/9eva7u3h' target=\"_blank\">https://wandb.ai/danlewis92-university-of-arizona/hw1p2/runs/9eva7u3h</a><br/> View project at: <a href='https://wandb.ai/danlewis92-university-of-arizona/hw1p2' target=\"_blank\">https://wandb.ai/danlewis92-university-of-arizona/hw1p2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/tmp/wandb/run-20250120_092252-9eva7u3h/logs</code>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"### Submit to kaggle competition using kaggle API (Uncomment below to use)\n#!kaggle competitions submit -c 11785-hw1p2-f24 -f ./submission.csv -m \"Test Submission\"\n\n### However, its always safer to download the csv file and then upload to kaggle","metadata":{"id":"LjcammuCxMKN","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:53.536501Z","iopub.execute_input":"2025-01-20T10:19:53.536762Z","iopub.status.idle":"2025-01-20T10:19:53.540645Z","shell.execute_reply.started":"2025-01-20T10:19:53.536734Z","shell.execute_reply":"2025-01-20T10:19:53.539673Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Don't worry about submissions, it's going to fail as the deadline has passed.\n# Aim to boost the train and val accuracy!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:19:53.541863Z","iopub.execute_input":"2025-01-20T10:19:53.542206Z","iopub.status.idle":"2025-01-20T10:19:53.552494Z","shell.execute_reply.started":"2025-01-20T10:19:53.542168Z","shell.execute_reply":"2025-01-20T10:19:53.551611Z"}},"outputs":[],"execution_count":36}]}